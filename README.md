# edge-llm-lab
Framework for systematic evaluation of low-latency inference techniques in small language models. Includes caching, FlashAttention, continuous batching, and speculative decoding strategies for on-device LLMs.
