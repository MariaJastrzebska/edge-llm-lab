\begin{table}[!ht]
\centering
\caption{Zbiorcze zestawienie metryk wydajnościowych i jakościowych dla testowanych modeli}
\label{tab:multi_model_metrics}
\resizebox{\linewidth}{!}{
\begin{tabular}{|l|c|c|c|c|c|r|}
\hline
\textbf{Model} & \textbf{GPT Score} & \textbf{Latencja (ms)} & \textbf{TPS} & \textbf{Pamięć (GB)} & \textbf{Energia (mW)} \\
\hline
granite3.2:2b-instruct-q8\_0 & 8.92 & 37785 & 16.2 & 0.00 & CPU: 434 \\ GPU: 38 \\ Total: 472 \\
\hline
llama3.2:3b-instruct-q4\_K\_M & 9.00 & 31436 & 18.3 & 0.00 & CPU: 1407 \\ GPU: 69 \\ Total: 1476 \\
\hline
granite4:micro & 8.28 & 37054 & 15.8 & 0.00 & CPU: 3938 \\ GPU: 19 \\ Total: 3957 \\
\hline
nemotron-mini:4b-instruct-q8\_0 & 1.36 & 3621 & 9.0 & 0.00 & CPU: 7875 \\ GPU: 12 \\ Total: 7887 \\
\hline
nemotron-mini:4b-instruct-q5\_K\_M & 1.40 & 4023 & 9.2 & 0.00 & CPU: 5169 \\ GPU: 13 \\ Total: 5182 \\
\hline
qwen2.5:3b-instruct-q4\_K\_M & 9.08 & 31594 & 20.5 & 0.00 & CPU: 2141 \\ GPU: 29 \\ Total: 2170 \\
\hline
granite3.2:2b-instruct-q4\_K\_M & 8.60 & 37692 & 23.3 & 0.00 & CPU: 504 \\ GPU: 0 \\ Total: 504 \\
\hline
granite3.1-dense:2b-instruct-q4\_K\_M & 7.92 & 29493 & 22.5 & 0.00 & CPU: 1755 \\ GPU: 35 \\ Total: 1790 \\
\hline
granite3.1-dense:2b-instruct-q8\_0 & 7.72 & 32092 & 16.6 & 0.00 & CPU: 683 \\ GPU: 10 \\ Total: 693 \\
\hline
\end{tabular}
}
\end{table}
