% Definicja kolorów i języka JSON dla pakietu listings
% Jeśli eval.tex jest dołączany poleceniem \input, upewnij się, że xcolor i listings są w preambule pliku głównego.
\definecolor{eclipseStrings}{RGB}{42,0,255}
\definecolor{eclipseKeywords}{RGB}{127,0,85}
\colorlet{numb}{magenta!60!black}
\definecolor{punct}{RGB}{206,128,57}

\expandafter\ifx\csname lstdefinelanguage\endcsname\relax
\else
\lstdefinelanguage{json}{
    basicstyle=\small\ttfamily,
    commentstyle=\color{eclipseKeywords},
    stringstyle=\color{eclipseStrings},
    numbers=left,
    numberstyle=\tiny,
    stepnumber=1,
    numbersep=8pt,
    showstringspaces=false,
    breaklines=true,
    frame=lines,
    string=[s]{"}{"},
    comment=[l]{//},
    morecomment=[s]{/*}{*/},
    literate=
     *{0}{{{\color{numb}0}}}{1}
      {1}{{{\color{numb}1}}}{1}
      {2}{{{\color{numb}2}}}{1}
      {3}{{{\color{numb}3}}}{1}
      {4}{{{\color{numb}4}}}{1}
      {5}{{{\color{numb}5}}}{1}
      {6}{{{\color{numb}6}}}{1}
      {7}{{{\color{numb}7}}}{1}
      {8}{{{\color{numb}8}}}{1}
      {9}{{{\color{numb}9}}}{1}
      {:}{{{\color{punct}{:}}}}{1}
      {,}{{{\color{punct}{,}}}}{1}
      {\{}{{{\color{punct}{\{}}}}{1}
      {\}}{{{\color{punct}{\}}}}}{1}
      {[}{{{\color{punct}{[}}}}{1}
      {]}{{{\color{punct}{]}}}}{1},
}
\fi

\section{Framework ewaluacyjny}

W celu oceny zaprojektowanego rozwiązania opracowano dedykowany, w pełni automatyczny framework ewaluacyjny, którego zadaniem jest empiryczna weryfikacja zachowania aplikacji oraz agentów bazujących na małych modelach językowych w warunkach możliwie najbardziej zbliżonych do rzeczywistego użycia. System ten nie testuje modelu w izolacji, lecz odtwarza kompletne scenariusze aplikacyjne: wielorundowe dialogi prowadzone przez pojedynczego agenta, w oparciu o ustalony prompt oraz narzucony format odpowiedzi (\emph{structured output}).

Framework został zaprojektowany jako narzędzie działające całkowicie offline, w sposób powtarzalny i deterministyczny, bez wykorzystania rzeczywistych danych użytkowników. Zaproponowano dwa podejścia ewaluacyjne: \textbf{z referencją} oraz \textbf{bez referencji}, które zostaną szczegółowo omówione w kolejnych podrozdziałach. Oba podejścia mają charakter półautomatyczny: scenariusze dialogowe oraz odpowiedzi pacjenta są przygotowywane manualnie, natomiast tzw. \emph{golden answers} generowane są automatycznie przez model referencyjny (w tym przypadku duży model językowy z dobrym zrozumieniem danych medycznych jak GPT), a następnie podlegają manualnej weryfikacji. Taki układ umożliwia wierne odwzorowanie rzeczywistego przebiegu rozmowy „pacjent–lekarz”, przy jednoczesnym zachowaniu kontroli eksperckiej nad poprawnością kliniczną i strukturalną odpowiedzi, a także nad poprawnością rozumowania (\emph{reasoningu}) małego modelu językowego, jego zdolnością do interpretacji instrukcji zawartych w promptcie, przestrzegania reguł w nim zdefiniowanych oraz poprawnego generowania odpowiedzi w narzuconym formacie \emph{structured output}.

Część odpowiedzialna za definicję scenariuszy pozostaje więc manualna, generacja referencji jest zautomatyzowana, a ich walidacja odbywa się z udziałem człowieka. Sam proces testowy – uruchamianie scenariuszy, rejestrowanie wyników i obliczanie metryk – jest w pełni zautomatyzowany. Dzięki temu proces ewaluacji pozostaje powtarzalny, zamknięty w obrębie środowiska badawczego oraz zgodny z wymaganiami lokalnego przetwarzania i ochrony danych medycznych.

W praktyce system umożliwia:
\begin{itemize}
\item symulację realistycznych scenariuszy użycia aplikacji,
\item automatyczną ocenę wielorundowych sesji dialogowych,
\item rejestrowanie odpowiedzi modelu w każdej rundzie interakcji,
\item zbieranie metryk jakościowych oraz wydajnościowych dla każdej interakcji,
\item powtarzalne porównywanie wariantów architektury agentowej, promptów oraz konfiguracji modeli,
\item dobór hiperparametrów oraz technik optymalizacji modeli w oparciu o mierzalne wyniki eksperymentalne.
\end{itemize}


Framework ten stanowi zaplecze eksperymentalne całej dalszej części pracy – wszystkie porównania modeli, konfiguracji inferencji oraz strategii agentowych opierają się na jednolitym, kontrolowanym środowisku testowym, co pozwala na jednoznaczną interpretację uzyskanych wyników.

\subsection{Metodologia ewaluacyjna dla modeli multi-agent multi-turn w kontekście medycznym}

Metodologia ewaluacji została zdefiniowana na poziomie pojedynczej rundy dialogu oraz całej sekwencji interakcji. Każdy scenariusz traktowany jest jako deterministyczny proces, w którym agent w kolejnych turach aktualizuje stan \texttt{current\_info}, formułuje pytanie w \texttt{missing\_info} oraz ustala globalny \texttt{status} procesu zbierania danych.

W ramach frameworku wyróżniono dwa komplementarne tryby oceny:

W obu trybach każda runda dialogu podlega wstępnej walidacji pod kątem spełnienia reguł zawartych w promptcie oraz w modelu Pydantic opisującym zachowanie agenta. Walidacja obejmuje poprawność składniową JSON oraz zgodność odpowiedzi ze schematem. Kryteria te stanowią wspólną warstwę strukturalnej kontroli poprawności i są niezależne od dostępności referencji.

\begin{itemize}
\item \textbf{ewaluacja z referencją} – każda runda posiada wzorcową odpowiedź (\emph{golden answer}), a odpowiedź badanego modelu jest oceniana względem referencji w wielu wymiarach jakościowych. Stosowane są metryki automatyczne (np.\ metryki NLP obliczane na poziomie poszczególnych pól i wartości JSON odpowiedzi) oraz mechanizm \emph{LLM-as-a-Judge}, w którym niezależny model porównuje odpowiedź badanego agenta bezpośrednio z odpowiedzą referencyjną.
\item \textbf{ewaluacja bez referencji} – odpowiedź oceniana jest bez użycia referencji przez LLM jako sędzię, w oparciu o własności odpowiedzi w kolejnych rundach dialogu. W tym trybie analizowana jest również efektywność zbierania danych: liczba rund wymaganych do skompletowania informacji, zdolność modelu do doprowadzenia dialogu do stanu \texttt{complete} oraz stabilność przebiegu interakcji. W tym wariancie mechanizm \emph{LLM-as-a-Judge} opiera się wyłącznie na regułach zawartych w promptcie, a nie na odpowiedzi referencyjnej.
\end{itemize}

Szczegółowa definicja metryk referencyjnych została przedstawiona w kolejnych podrozdziałach.

Oba tryby działają na podobnych logach dialogowych i opierają się na wspólnym rdzeniu kontraktu danych (tj.\ identycznym formacie rejestracji przebiegu dialogu i odpowiedzi agenta). Różnią się natomiast warstwą oceny: zestawem metryk oraz kryteriami walidacji, które są specyficzne dla trybu referencyjnego lub bez referencyjji.

\subsection{Struktura Logów Ewaluacyjnych (JSON)}

Wyniki ewaluacji agentów medycznych są rejestrowane w ustrukturyzowanych plikach JSON, co umożliwia precyzyjną analizę techniczną i jakościową. Plik zawiera listę obiektów w kluczu \texttt{evaluations}, gdzie każdy wpis reprezentuje kompletną sesję testową konkretnego modelu. Poniżej przedstawiono schemat kluczowych sekcji logu (Listing \ref{lst:log_example_final}), ilustrujący jak system integruje dane o wydajności z jakością odpowiedzi.

\begin{lstlisting}[language=json, caption={Struktura logu ewaluacyjnego (zgodna z formatem bazy danych)}, label={lst:log_example_final}]
{
  "evaluations": [
    {
      "session_timestamp": "2025-12-29_17-40-32",
      "model_name": "granite3.2:2b-instruct-q8_0",
      "evaluator_model_name": "gpt-4o-mini",
      "evaluation_type": "referenced",
      "optimisations": [ {} ],
      "parameters": {
        "cot_prompt_path": "/Users/.../prompts/constant_data_en.txt",
        "context_size": 8192,
        "temperature": 0.1,
        "top_p": 1.0,
        ...
      },
      "agent_type": "constant_data_en",
      "use_cache": true,
      "tools": [ { "type": "function", "function": { "name": "...", "parameters": { ... } } } ],
      "rounds": [
        {
          "round": 1,
          "context": [ ... ],
          "llm_response": {
            "thoughts": [ { "thought": "...", "action": "...", "action_input": "..." } ],
            "status": "incomplete",
            "missing_info": [ { "field": "...", "question": "..." } ]
          },
          "reference_response": { ... },
          "latency_breakdown": {
            "total_ms": 1150,
            "prompt_eval_ms": 320,
            "token_generation_ms": 830,
            "tokens": { "prompt_tokens": 1240, "completion_tokens": 150, "throughput": 48.2 }
          },
          "resource_differences": {
            "memory": { "ram_delta_gb": 0.12, "swap_delta_gb": 0.02, ... },
            "energy": { "cpu_power_delta_mw": 480, "gpu_power_delta_mw": 20, ... }
          },
          "metrics": {
            "json_validity": { "score": 1.0 },
            "bert_score": { "f1": 0.92, ... },
            "gpt_judge": { 
              "score": 9.0, 
              "category_scores": { "JSON Correctness": 10, ... },
              "strengths": [ ... ],
              "weaknesses": [ ... ]
            }
          }
        }
      ]
    }
  ]
}
\end{lstlisting}

Podstawowe komponenty struktury obejmują:
\begin{itemize}
    \item \textbf{Metadane Sesji}: Parametry generowania, nazwa modelu oraz definicje dostępnych narzędzi (\textit{tools}).
    \item \textbf{Latency Breakdown}: Szczegółowy podział czasu odpowiedzi na fazę przetwarzania promptu (\textit{prefill}) oraz generowania tokenów.
    \item \textbf{Resource Monitoring}: Rejestracja delty zużycia pamięci operacyjnej oraz poboru mocy (CPU/GPU delta), co jest kluczowe dla optymalizacji brzegowej.
\end{itemize}

\subsection{LLM jako sędzia: Protokół 5 Filarów}

W obliczu subiektywizmu metryk tekstowych w medycynie, wdrożono protokół \textit{LLM-as-a-Judge}. Wykorzystuje on wysokiej klasy model (np. GPT-4o-mini) do oceny agenta w skali 1--10 według pięciu krytycznych kryteriów:

\begin{enumerate}
\item \textbf{JSON Correctness} -- Weryfikacja zgodności ze schematem Pydantic. Błąd struktury w systemie medycznym uniemożliwia automatyczną analizę danych.
\item \textbf{Tool Call Correctness} -- Ocena trafności wywołania narzędzi (np. \texttt{send\_medical\_data}). System sprawdza, czy agent nie wywołuje funkcji przedwcześnie.
\item \textbf{Reasoning Logic} -- Analiza sekcji \texttt{thoughts}. Sędzia sprawdza, czy rozumowanie agenta jest logicznie spójne z dostarczonymi faktami medycznymi.
\item \textbf{Question Naturalness} -- Ocena empatii i stylu komunikacji. W kontekście zdrowia pacjent musi czuć się komfortowo podczas zbierania danych.
\item \textbf{Context Relevance} -- Weryfikacja pamięci operacyjnej modelu. Agent musi unikać redundancji i pamiętać fakty podane w poprzednich turach dialogu.
\end{enumerate}

\subsection{Zastosowane Narzędzia i Stos Technologiczny}
Implementacja frameworku oraz logika agentowa zostały zrealizowane w języku \textbf{Python 3.x}, co zapewniło elastyczność w integracji modeli LLM z bibliotekami do analizy danych. Kluczowe komponenty stosu technologicznego obejmują:

\begin{itemize}
    \item \textbf{llama-cpp-python} -- Wykorzystany jako główny silnik inferencyjny (backend \texttt{llama.cpp}), umożliwiający wydajne uruchamianie modeli w formacie GGUF na procesorach CPU i akceleratorach brzegowych (np. Apple Metal).
    \item \textbf{Pydantic} -- Zastosowany do definiowania restrykcyjnych kontraktów danych (\textit{Data Schemas}) oraz automatycznej walidacji wyjść strukturalnych agenta. Każda odpowiedź jest weryfikowana pod kątem poprawności typów i obecności wymaganych pól klinicznych.
    \item \textbf{Neptune AI} -- Służy jako centralna platforma \textbf{MLOps} do zarządzania eksperymentami. System automatycznie loguje każdą sesję, przechowując parametry (\textit{temperature}, \textit{top\_p}), wersje kodów promptów oraz artefakty w postaci logów JSON. Dashboardy Neptune AI umożliwiają dynamiczne porównywanie wydajności różnych poziomów kwantyzacji (np. Q4 vs Q8).
    \item \textbf{Matplotlib i Numpy} -- Wykorzystane do statystycznej obróbki wyników oraz generowania wykresów trendów jakościowych per-runda.
\end{itemize}

Dzięki takiemu doborowi narzędzi, proces badawczy (Rys. \ref{rys:wytlumaczenie}) jest w pełni replikowalny i pozwala na precyzyjną ocenę wpływu optymalizacji na zachowanie modelu.

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.6\linewidth]{rysunki/eval/wytlumaczenie.png}
    \caption{Szczegółowy raport sędziowski (GPT Judge) ilustrujący metodologię oceny jakościowej.}
    \label{rys:wytlumaczenie_intro}
\end{figure}

\subsection{Ewaluacja z referencją} 

Tryb \textbf{z referencją} wykorzystuje wcześniej przygotowane, pełne wielorundowe konwersacje pełniące rolę złotego standardu przebiegu rozmowy agenta. Docelowo konwersacje referencyjne powinny być opracowywane z ekspertem dziedzinowym oraz bazować na obserwacjach z realnych interakcji użytkowników.Na potrzeby niniejszych eksperymentów konwersacje referencyjne były konstruowane w trybie interakcyjnym z wykorzystaniem ChatGPT w roli pacjenta, natomiast po stronie agenta stosowany był dokładnie ten sam prompt oraz te same reguły \emph{structured output}, które później otrzymywały modele testowane. W każdej turze ręcznie wprowadzano odpowiedzi „pacjenta”, symulując realistyczny przebieg rozmowy, podczas gdy zachowanie agenta było w pełni determinowane przez zaprojektowany prompt i schemat decyzyjny.

Wygenerowane w ten sposób dialogi były następnie poddawane ręcznej weryfikacji i korekcie w celu zapewnienia spójności scenariusza, poprawności logicznej oraz ścisłej zgodności z przyjętymi schematami \emph{structured output}. Tak skonstruowane referencje zachowują tę samą dynamikę i ograniczenia, którym podlegają modele ewaluowane, a jednocześnie umożliwiają pełną kontrolę nad przebiegiem rozmowy oraz deterministyczność eksperymentów.

Ewaluacja jest realizowana per-runda: w każdej turze testowany model otrzymuje bieżący prompt wraz z instrukcją generowania \textit{structured output} oraz kontekst rozmowy (w tym fragment konwersacji referencyjnej podawany w schemacie \textit{sliding window}, zależnie od rundy). Ocenie podlega odpowiedź modelu w danej rundzie, porównywana z odpowiedzą referencyjną oraz analizowana metrykami automatycznymi, w tym mechanizmem \textit{LLM-as-a-Judge}. Podejście to umożliwia:

\begin{itemize}
    \item deterministyczną weryfikację poprawności strukturalnej odpowiedzi (parsowalność JSON, kompletność wymaganych pól, zgodność struktury i typów, poprawność wartości przekazywanych w argumentach),
    \item ilościową ocenę podobieństwa odpowiedzi do referencji z wykorzystaniem metryk znakowych i tokenowych (m.in. Levenshtein similarity, Jaccard similarity, BLEU, ROUGE, METEOR),
    \item ocenę zgodności semantycznej odpowiedzi z referencją przy użyciu metryk embeddingowych (BERTScore Precision i Recall),
    \item wielowymiarową ocenę jakości zachowania agenta przy użyciu \textit{LLM-as-a-Judge}, obejmującą m.in. poprawność strukturalną, logikę rozumowania, trafność kontekstową, naturalność pytania oraz poprawność wywołań narzędzi,
    \item analizę stabilności zachowania modelu poprzez agregację wyników metryk w kolejnych rundach dialogu i identyfikację degradacji jakości wraz z narastaniem kontekstu,
    \item korelację jakości odpowiedzi z metadanymi procesu inferencji, obejmującymi m.in. latencję, przepustowość generacji, zużycie pamięci operacyjnej oraz szacowany wpływ na zużycie energii, rejestrowanymi w logach ewaluacyjnych.
\end{itemize}

\subsection{Ewaluacja bez referencji} 

W trybie \textbf{bez referencji}, framework przełącza się w tryb pełnej symulacji. Zamiast statycznej odpowiedzi referencyjnej, wykorzystywany jest \textbf{Patient Simulator} (oparty na GPT-4o-mini), który dynamicznie reaguje na pytania badanego modelu. Pozwala to na badanie agenta w warunkach "otwartego świata", gdzie ścieżka dialogu nie jest sztywno narzucona.

Główne metryki w tym trybie obejmują:
\begin{itemize}
    \item \textbf{Efficiency} -- Liczba rund dialogowych potrzebnych do przejścia ze stanu \texttt{incomplete} do \texttt{complete}.
    \item \textbf{Success Rate} -- Odsetek sesji zakończonych poprawnym zapisem danych bez utraty stabilności konwersacji.
    \item \textbf{Stability} -- Zdolność modelu do unikania pętli (powtarzania tych samych pytań) przy braku wzorcowej ścieżki.
\end{itemize}

Ten tryb ewaluacji jest kluczowy dla oceny rzeczywistej użyteczności klinicznej, gdyż weryfikuje odporność modelu na dygresje i nieprecyzyjne odpowiedzi "cyfrowego pacjenta".

\subsection{Wyniki ewaluacji z poszczególnych rund}
Fundamentem analizy jakościowej jest szczegółowa ewaluacja każdej rundy dialogu. System generuje granularne statystyki, które pozwalają na diagnostykę zachowania modelu w konkretnych punktach interakcji. Na rysunku \ref{rys:jedna runda} przedstawiono reprezentatywny wynik dla modelu \texttt{Llama-3.2-3B} w 4. rundzie zbierania danych.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.85\linewidth]{rysunki/eval/jedna runda.png}
    \caption{Szczegółowe wyniki metryk dla pojedynczej rundy (Llama-3.2-3B, Runda 4). Wykres prezentuje zestawienie metryk technicznych (JSON validity), lingwistycznych (BERTScore, ROUGE) oraz sędziowskich.}
    \label{rys:jedna runda}
\end{figure}

Wykres słupkowy (Rys. \ref{rys:jedna runda}) ilustruje profil wydajnościowy agenta w danej turze. Widoczna jest wysoka poprawność strukturalna (\texttt{json\_validity} równe 1.0) oraz bardzo dobre dopasowanie semantyczne (\texttt{BERTScore} powyżej 0.9). Takie zestawienie pozwala na natychmiastową identyfikację momentu, w którym model zaczyna tracić spójność lub halucynować, co jest kluczowe przy testowaniu długich sesji medycznych.

Dopełnieniem danych ilościowych jest warstwa jakościowa generowana przez mechanizm \textit{LLM-as-a-Judge}. Rysunek \ref{rys:wytlumaczenie} prezentuje szczegółowy raport uzasadniający przyznane noty.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.5\linewidth]{rysunki/eval/wytlumaczenie.png}
    \caption{Raport szczegółowy GPT Judge. System dostarcza tekstowe uzasadnienie dla każdego z 5 filarów, wskazując np. na subtelne rozbieżności w polu \texttt{missing\_info}.}
    \label{rys:wytlumaczenie}
\end{figure}

Jak widać na przykładzie (Rys. \ref{rys:wytlumaczenie}), sędzia potrafi wychwycić błędy logiczne, takie jak nieuzasadnione uznanie statusu za \texttt{INCOMPLETE} mimo zebrania wymaganych pól, lub ocenić naturalność sformułowanego pytania o grupę krwi. Takie logi są generowane systematycznie dla wszystkich testowanych modeli i wszystkich rund, tworząc potężną bazę danych do optymalizacji promptów i doboru technik kwantyzacji.

Analiza tak szczegółowych danych pozwoliła na zauważenie istotnego trendu. Chociaż znalezienie małych modeli zdolnych do poprawnego generowania \textit{structured output} jest możliwe, większość ogólnodostępnych modeli w tej klasie wielkościowej (1--4B) wykazuje niską stabilność składniową bez zewnętrznego wymuszania (gramatyk). Modele wyłonione w procesie selekcji często utrzymują poprawność formatu JSON, jednak ich "logika medyczna" oraz zdolność do podążania za złożonymi instrukcjami klinicznymi zaczynają degradować wraz z narastaniem kontekstu. Zjawisko to byłoby niemożliwe do wykrycia przy użyciu wyłącznie metryk znakowych, co potwierdza konieczność stosowania wielowymiarowej oceny sędziowskiej.

\section{Analiza porównawcza małych modeli językowych na urządzeniach brzegowych}
Dobór odpowiedniego modelu językowego do konkretnego przypadku użycia stanowi istotne wyzwanie badawcze i inżynierskie. Wymaga on jednoczesnego uwzględnienia wielu czynników, takich jak: architektura modelu, rozmiar i liczba parametrów, charakterystyka danych pretrenowania, zdolności rozumowania (reasoning), wsparcie dla mechanizmów takich jak function calling, a także ograniczenia sprzętowe i energetyczne wynikające z docelowego środowiska uruchomieniowego.

W ostatnich latach obserwowany jest dynamiczny rozwój architektur oraz technik optymalizacyjnych, które w istotny sposób wpływają zarówno na jakość generowanych odpowiedzi, jak i na efektywność inferencji. Przykładem są architektury typu Mixture of Experts (MoE), w których tylko podzbiór parametrów aktywowany jest dla danego tokenu, co pozwala na zwiększenie zdolności modelu bez proporcjonalnego wzrostu kosztów obliczeniowych.

Równolegle rozwijane są alternatywne architektury sekwencyjne, takie jak modele oparte na mechanizmie state space models (np. Mamba), które rezygnują z klasycznej uwagi (self-attention) na rzecz liniowej złożoności względem długości sekwencji. Ma to istotne znaczenie w kontekście inferencji na urządzeniach brzegowych, gdzie pamięć operacyjna oraz przepustowość energetyczna są silnie ograniczone.

\subsection{Dobór modeli językowych do use casu medycznego}
Kluczowym elementem analizowanego przypadku użycia była zdolność małego modelu językowego do generowania odpowiedzi w ściśle narzuconym formacie. Obejmowało to w szczególności:
\begin{itemize}
    \item tworzenie logicznych kroków rozumowania (\textit{Chain of Thought}),
    \item strukturyzowanie odpowiedzi zgodnie z zadanym schematem JSON,
    \item obsługę mechanizmów function callingu, umożliwiających bezpośrednią integrację z warstwą medyczną.
\end{itemize}

W pierwszym etapie selekcji, ze względu na specyfikę pracy bez mechanizmów wymuszania struktury (\textit{grammars}), zastosowano metodę \textbf{prób i błędów} w celu wyłonienia modeli wykazujących bazową stabilność w generowaniu JSON. Empirycznie potwierdzono, że modele posiadające natywne wsparcie dla mechanizmu \textbf{tool calling} (wywoływania funkcji) charakteryzują się znacznie większą szansą na poprawne generowanie odpowiedzi strukturalnych również w ogólnych zadaniach agentowych. Do końcowej ewaluacji wybrano modele o rozmiarze nieprzekraczającym 6 GB w wersji nieskwantyzowanej (z przedziału 1--8B parametrów), które przeszły pomyślnie wstępne testy składniowe. Do monitorowania ich wydajności wykorzystano wcześniej opisaną metodologii oraz platformę Neptune AI (Rys. \ref{fig:neptune-dashboard}).

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.95\linewidth]{"rysunki/eval/neptune ai".png}
    \caption{Dashboard platformy Neptune AI prezentujący rezultaty serii ewaluacyjnej run\_070. Widoczna agregacja parametrów technicznych, zużycia zasobów oraz wyników jakościowych GPT Score dla różnych architektur.}
    \label{fig:neptune-dashboard}
\end{figure}

W celu wyłonienia modeli najlepiej radzących sobie z wysokoryzykownym kontekstem medycznym, przeprowadzono szczegółową analizę porównawczą, zestawiając jakość sędziowską z efektywnością energetyczną i latencją. 

Na podstawie przeprowadzonych testów wyłoniono grupę modeli o najwyższym potencjale wdrożeniowym. Proces selekcji wspierany był przez wielowymiarową analizę radarową (Rys. \ref{fig:radar-analysis}), która pozwoliła na jednoczesną ocenę jakości merytorycznej (GPT Score), szybkości reakcji (Latency) oraz efektywności energetycznej. 

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.85\linewidth]{rysunki/eval/run_070_full/all_models_with_reference_2026-01-18_00-23-07_radar.png}
    \caption{Wielowymiarowa analiza porównawcza modeli (Radar Chart). Wykres obrazuje kompromis między jakością merytoryczną (GPT Score) a wydajnością energetyczną i latencją.}
    \label{fig:radar-analysis}
\end{figure}

Szczegółowe parametry techniczne testowanych jednostek zestawiono w tabeli \ref{tab:model_details}.

\begin{table}[h!]
\centering
\caption{Szczegóły techniczne i profil energetyczny testowanych modeli}
\label{tab:model_details}
\resizebox{\linewidth}{!}{
\begin{tabular}{|l|l|c|c|r|p{4cm}|}
\hline
\textbf{Model} & \textbf{Architektura} & \textbf{Params} & \textbf{Rozmiar} & \textbf{Kwant.} & \textbf{Energia (mW)} \\
\hline
granite3.2:2b-instruct-q8:0 & granite & 2.5B & 3.0 GB & Q8\_0 & CPU: 409mW \newline GPU: 10mW \newline Total: 419mW \\
\hline
llama3.2:3b-instruct-q4:K\_M & llama & 3.2B & 2.1 GB & Q4\_K\_M & CPU: 781mW \newline GPU: 39mW \newline Total: 820mW \\
\hline
granite4:micro & granite & 0.4B & 2.1 GB & Q8\_0 & CPU: 3841mW \newline GPU: 117mW \newline Total: 3958mW \\
\hline
nemotron-mini:4b-instruct-q8:0 & nemotron & 4.0B & 4.0 GB & Q8\_0 & CPU: 467mW \newline GPU: 30mW \newline Total: 496mW \\
\hline
nemotron-mini:4b-instruct-q5:K\_M & nemotron & 4.0B & 3.0 GB & Q5\_K\_M & CPU: 319mW \newline GPU: 23mW \newline Total: 343mW \\
\hline
qwen2.5:3b-instruct-q4:K\_M & qwen & 3.0B & 2.0 GB & Q4\_K\_M & CPU: 2763mW \newline GPU: 18mW \newline Total: 2781mW \\
\hline
\end{tabular}
}
\end{table}
Podsumowując, analiza danych pozwala na sformułowanie następujących wniosków:
\begin{itemize}
    \item \textbf{Zwycięzca balansu}: Model \texttt{Granite 3.2 2B (Q8)} wykazuje optymalny stosunek jakości sędziowskiej (stabilny GPT Score > 7.5) do zużycia zasobów ($419$ mW). Jest to kluczowe, gdyż wiele szybszych modeli klasy 1B generuje odpowiedzi o niskiej wartości merytorycznej (GPT Score < 3), co czyni ich wydajność bezużyteczną w kontekście klinicznym.
    \item \textbf{Krytyczny wpływ SWAP}: Modele przekraczające 3GB parametrów (np. Nemotron-mini) mimo wysokiej jakości, powodują agresywne wykorzystanie SWAP (Rys. \ref{fig:health-check}). Skutkuje to „szarpaniem” interfejsu użytkownika, co dyskwalifikuje je z płynnych zastosowań mobilnych mimo poprawnej merytoryki.
    \item \textbf{Stabilność pomiarowa}: Stała stabilność CPU na poziomie 100\% (brak throttlingu) we wszystkich testach run\_070 potwierdza, że uzyskane wyniki latencji są rzetelne i nie wynikają z chwilowego przegrzania urządzenia, lecz z wydajności samej architektury modelu.
    \item \textbf{Wpływ tool calling}: Modele z natywnym wsparciem wywoływania funkcji charakteryzują się znacznie stabilniejszymi wynikami w protokole 5 Filarów, rzadziej „gubią” strukturę JSON przy rosnącym oknie kontekstowym.
\end{itemize}

\begin{enumerate}
    \item \textbf{Wielowymiarowa Analiza Radarowa} (Rys. \ref{fig:radar-analysis}) – wizualizuje balans między czterema domenami: jakością (GPT Score), szybkością startową (Latency), rozmiarem modelu oraz efektywnością energetyczną. Pozwala odrzucić modele, które dominują w jednej kategorii (np. są bardzo małe), ale zawodzą w najważniejszej — poprawności odpowiedzi.
    
    \begin{figure}[htbp]
        \centering
        \includegraphics[width=0.85\linewidth]{rysunki/eval/run_070_full/all_models_with_reference_2026-01-18_00-23-07_radar.png}
        \caption{Wielowymiarowa analiza porównawcza modeli (Radar Chart). Wykres obrazuje kompromis między jakością merytoryczną (GPT Score) a wydajnością energetyczną i latencją.}
        \label{fig:radar-analysis}
    \end{figure}
    
    \item \textbf{Analiza Wydajności: TPS vs Latencja} (Rys. \ref{fig:performance-analysis}) – wykres bąbelkowy pokazujący realną płynność pracy. Idealny kandydat mobilny znajduje się v lewym górnym rogu (niska latencja startowa, wysoka przepustowość tokenów na sekundę). Średnia wartość ~15 TPS dla modeli 2-3B jest uznawana za próg pełnego komfortu interakcji.
    
    \begin{figure}[h!]
        \centering
        \includegraphics[width=0.9\linewidth]{rysunki/eval/run_070_full/all_models_all_optimizations_mobile_performance_analysis_2026-01-18_00-23-07.png}
        \caption{Analiza wydajnościowa: Przepustowość (TPS) vs Latencja. Rozmiar bąbelka odpowiada fizycznemu rozmiarowi modelu na dysku.}
        \label{fig:performance-analysis}
    \end{figure}
    
    \item \textbf{Resource Health Check} (Rys. \ref{fig:health-check}) – trójdzielna diagnostyka (RAM, SWAP, Throttling). \textbf{Ważna uwaga metodologiczna:} Wszystkie pomiary wydajności przeprowadzono w środowisku deweloperskim na komputerze \textbf{MacBook Air}, co pozwala na ocenę potencjału modeli przed wdrożeniem na iOS. Diagnostyka ta pozwala zrozumieć kluczowe zjawiska:
    \begin{itemize}
        \item \textbf{RAM Usage (Wykres 1)}: Zestawienie realnego zapotrzebowania modelu z dwiema liniami granicznymi. \textbf{Limit MacBooka (16GB)} oraz \textbf{Limit iPhone'a (8GB)}. Słupki modeli przekraczających limit 8GB sygnalizują konieczność ekstremalnej kompresji (kwantyzacji) przed migracją na telefon.
        \item \textbf{Memory Pressure / SWAP (Wykres 2)}: SWAP pełni rolę "bezpiecznika" systemowego. Odnotowane wartości GPU Power (widoczne w zagregowanych tabelach) wynikają z faktu, że architektura Apple Silicon automatycznie wykorzystuje akcelerację \textbf{Metal (GPU)} do operacji na tensorach, nawet jeśli nie została ona jawnie wymuszona przez użytkownika.
        \item \textbf{Stabilność CPU (Wykres 3)}: Widoczna na wykresach \textbf{100\% stabilność} (brak throttlingu) przy jednoczesnym wysokim SWAP wynika z charakteru zadania. Inferencja LLM jest procesem \textbf{memory-bound} (ograniczonym przez przepustowość pamięci). Procesor nie przegrzewa się do progu throttlingu, ponieważ większość cykli spędza na "oczekiwaniu" na dane wczytywane z wolnej pamięci wirtualnej/SSD. Jest to stan niepożądany, gdyż nominalne zegary maskują drastyczne spowolnienie generacji tekstu.
    \end{itemize}

    \begin{figure}[h!]
        \centering
        \includegraphics[width=0.85\linewidth]{rysunki/eval/run_070_full/resource_health_check_unified_2026-01-18_02-36-24.png}
        \caption{Diagnostyka zdrowia systemu (Resource Health Check). Zestawienie użycia RAM (z uwzględnieniem limitów MacBook/iPhone), ciśnienia na SWAP oraz stabilności częstotliwości CPU.}
        \label{fig:health-check}
    \end{figure}
\end{enumerate}
\subsection{Dynamika użycia zasobów w czasie}
Oprócz zagregowanych danych, kluczowe dla optymalizacji mobilnej jest zrozumienie, jak model obciąża system w miarę wzrostu okna kontekstowego. Analiza liniowych przebiegów pozwala na identyfikację wycieków pamięci lub momentów, w których system operacyjny zaczyna agresywnie korzystać z partycji wymiany (SWAP).

W celu wizualizacji tego zjawiska zestawiono dwa kontrastujące profile dynamiki zasobów (Rys. \ref{fig:timeline-granite} oraz \ref{fig:timeline-llama}). Skupiono się na sekcji \textit{RAM \& Memory Pressure}, która obrazuje dwa kluczowe scenariusze:
\begin{itemize}
    \item \textbf{Scenariusz A: SWAP > RAM (Memory Spill)}: Stan krytyczny, w którym system przenosi dane na dysk SSD. Mimo stabilności CPU (100\%), realna generacja tekstu staje się skokowa. Jest to sytuacja, w której model "przelewa się" poza fizyczną pamięć urządzenia.
    \item \textbf{Scenariusz B: RAM > SWAP (Status Zdrowy)}: Pożądany stan, gdzie wzrost RAM (związany z kontekstem) odbywa się przy minimalnym SWAP. Oznacza to, że model "mieści się" w pamięci, a wydajność ograniczają jedynie bramki logiczne procesora.
    \item \textbf{Punkt Przecięcia (Crossover)}: Moment, w którym linia SWAP przecina linię RAM, wyznacza pragmatyczną granicę użyteczności urządzenia dla danego modelu.
\end{itemize}

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.85\linewidth]{rysunki/eval/run_070_full/throttling_timeline_granite3_2_2b-instruct-q8_0_2026-01-18_00-23-07.png}
    \caption{Dynamika zasobów: Granite 3.2 2B (Q8). Widoczna wysoka stabilność użycia RAM oraz minimalne wykorzystanie SWAP nawet przy dłuższej konwersacji.}
    \label{fig:timeline-granite}
\end{figure}

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.85\linewidth]{rysunki/eval/run_070_full/throttling_timeline_llama3_2_3b-instruct-q4_K_M_2026-01-18_00-23-07.png}
    \caption{Dynamika zasobów: Llama 3.2 3B (Q4). Zaobserwować można liniowy wzrost zajętości pamięci RAM oraz stopniowe narastanie użycia SWAP (Memory Pressure) wraz z kolejnymi rundami dialogu.}
    \label{fig:timeline-llama}
\end{figure}

Analiza ta pozwala stwierdzić, że mechanizm uwalniania pamięci (\textit{Resource Cleanup}) działa poprawnie, a system brzegowy utrzymuje pełną stabilność zegarów procesora nawet przy pełnym obciążeniu wszystkich rdzeni podczas inferencji. Jednakże większe modele (klasy 3B i wyżej) wykazują naturalną tendencję do "wypychania" mniej istotnych procesów systemowych do SWAP w miarę nasycania okna kontekstowego. Zjawisko to jest kluczowe przy projektowaniu interfejsu aplikacji mobilnej — nagłe narastanie SWAP może prowadzić do mikro-przycięć animacji (\textit{jank}), co dyskwalifikuje dany model z zastosowań wymagających najwyższej responsywności UI.

Na podstawie powyższych danych, framework pozwolił na obiektywne przejście od szerokiej listy modeli open-source do wąskiej grupy, która gwarantuje bezpieczeństwo i użyteczność w restrykcyjnym środowisku brzegowym.

\subsection{Wyniki jakościowe testów na urządzeniach mobilnych}
W fazie walidacji końcowej model zwycięzca został uruchomiony na urządzeniu testowym (iPhone/iPad) przy użyciu frameworku \textit{flutter-llama}. Empiryczne testy potwierdziły, że mimo braku \textit{structured output enforcement} na poziomie silnika, modele takie jak Granite-3.2b wykazują zadziwiającą stabilność składniową nawet przy głębokim oknie kontekstowym (powyżej 8 rund dialogu).
