if __name__ == "__main__":
    from referenced_clean import EvalModelsReferenced
    from unreferenced_clean import EvalModelsUnreferenced
    from base_eval import BaseEvaluation, Agent
    
    # === KONFIGURACJA GLOBALNYCH USTAWIE≈É ===
    print("\n KONFIGURACJA STEP-BY-STEP EVALUATION")
    print("="*60)
    
    # U≈ºyj sta≈Çego agenta
    agent_type_enum = Agent.CONSTANT_DATA_EN
    print(f"ü§ñ Agent: {agent_type_enum.value}")
    
    # 1. Wyb√≥r modeli do testowania
    print("\n  WYB√ìR MODELI DO TESTOWANIA:")
    print("1. Wszystkie modele z config.yaml")
    print("2. Tylko modele z tested: true bez wynik√≥w w logach")
    mode_choice = input("Wybierz tryb: 1 - Wszystkie modele z config, 2 - Tylko tested: true bez wynik√≥w: ")
    
    # 2. Czy automatycznie pobieraƒá modele
    print("\n POBIERANIE MODELI:")
    auto_install = input("Czy automatycznie pobieraƒá brakujƒÖce modele? (y/n): ").lower().strip()
    install_choice = "y" if auto_install in ["y", "yes", "tak"] else "n"
    
    print("\n" + "="*60)
    print(" KONFIGURACJA ZAKO≈ÉCZONA")
    print("="*60)
    
    # Najlepsza optymalizacja z analizy
    best_optimization = [
        {},  # Baseline
        {"--flash-attn": None, "--cont-batching": None},  # Najlepsza kombinacja (-24.5% latencji)
    ]
    
    print(f"üöÄ Optymalizacje do testowania:")
    print(f"   1. Baseline (bez optymalizacji)")
    print(f"   2. Flash Attention + Continuous Batching (najlepsza: -24.5% latencji)")
    
    # Pobierz modele do testowania
    if mode_choice == "1":
        models_to_evaluate = EvalModelsReferenced.get_truly_untested_models(agent_type_enum.value, "referenced", only_tested_true=False)
        print("  Tryb: Wszystkie modele z config")
    else:
        models_to_evaluate = EvalModelsReferenced.get_truly_untested_models(agent_type_enum.value, "referenced", only_tested_true=True)
        print("  Tryb: Tylko tested: true bez wynik√≥w")
    
    if not models_to_evaluate:
        print(f"‚ùå Brak modeli do testowania dla agenta {agent_type_enum.value}")
        exit(1)
        
    total_models = len(models_to_evaluate)
    print(f" Znaleziono {total_models} modeli do testowania")

    # === PƒòTLA PRZEZ MODELE ===
    for i, model_name in enumerate(models_to_evaluate, 1):
        print(f"\n{'='*80}")
        print(f" MODEL {i}/{total_models}: {model_name}")
        print(f"{'='*80}")

        # Sprawd≈∫ dostƒôpno≈õƒá modelu (u≈ºywaj globalnego ustawienia)
        if not BaseEvaluation.check_model_availability(model_name, install_choice=install_choice):
            print(f" Pomijam model {model_name}...")
            continue
            
        print(f" Model dostƒôpny: {model_name}")
        print(f" Agent: {agent_type_enum.value}")
        
        # === REFERENCED EVALUATION ===
        print(f"\n REFERENCED EVALUATION - {model_name}")
        print(f"-" * 60)
        
        try:
            referenced_evaluator = EvalModelsReferenced(
                model_name=model_name,
                agent=agent_type_enum
            )
            
            # Uruchom referenced z najlepszymi optymalizacjami (tylko logi)
            referenced_evaluator.pipeline_eval_model(
                mode="logs_only",  # Tylko logi, bez wizualizacji
                use_cache=True,
                optimisations=best_optimization
            )
            
            print(f" Referenced evaluation zako≈Ñczona dla {model_name}")
            del referenced_evaluator  # Zwolnij pamiƒôƒá
            
        except Exception as e:
            print(f"‚ùå B≈ÇƒÖd w referenced evaluation dla {model_name}: {e}")
            continue  # Przejd≈∫ do nastƒôpnego modelu
        
        # === UNREFERENCED EVALUATION ===
        print(f"\nüè• UNREFERENCED EVALUATION - {model_name}")
        print(f"-" * 60)
        
        try:
            unreferenced_evaluator = EvalModelsUnreferenced(
                model_name=model_name,
                agent=agent_type_enum
            )
            
            # Uruchom unreferenced z najlepszymi optymalizacjami (tylko logi)
            unreferenced_evaluator.pipeline_eval_model(
                mode="logs_only",  # Tylko logi, bez wizualizacji
                use_cache=True,
                optimisations=best_optimization
            )
            
            print(f" Unreferenced evaluation zako≈Ñczona dla {model_name}")
            del unreferenced_evaluator  # Zwolnij pamiƒôƒá
            
        except Exception as e:
            print(f"‚ùå B≈ÇƒÖd w unreferenced evaluation dla {model_name}: {e}")
        
        # === OZNACZ MODEL JAKO TESTED ===
        try:
            from model_config_loader import mark_model_as_tested
            mark_model_as_tested(agent_type_enum.value, model_name)
            print(f"  Model {model_name} oznaczony jako tested: true")
        except Exception as e:
            print(f"  Nie uda≈Ço siƒô oznaczyƒá modelu jako tested: {e}")
        
        print(f"üéâ Model {model_name} zako≈Ñczony - referenced + unreferenced")

    # === PODSUMOWANIE ===
    print(f"\n{'='*80}")
    print(f" EWALUACJA STEP-BY-STEP ZAKO≈ÉCZONA!")
    print(f"{'='*80}")
    print(f" Przetestowano {total_models} modeli")
    print(f"  Ka≈ºdy model: Referenced ‚Üí Unreferenced")
    print(f"‚ö° Optymalizacje: Baseline + Flash Attention + Continuous Batching")
    print(f"üìù Tryb: Tylko logi (bez wizualizacji)")
    print(f"\nüí° Aby wygenerowaƒá wizualizacje, uruchom:")
    print(f"   - referenced_clean.py z mode='viz_only'")
    print(f"   - unreferenced_clean.py z mode='viz_only'")