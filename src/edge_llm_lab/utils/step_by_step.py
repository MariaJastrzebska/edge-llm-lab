if __name__ == "__main__":
    from referenced_clean import EvalModelsReferenced
    from unreferenced_clean import EvalModelsUnreferenced
    from base_eval import BaseEvaluation, Agent
    
    # === KONFIGURACJA GLOBALNYCH USTAWIEÅƒ ===
    print("\nğŸ” KONFIGURACJA STEP-BY-STEP EVALUATION")
    print("="*60)
    
    # UÅ¼yj staÅ‚ego agenta
    agent_type_enum = Agent.CONSTANT_DATA_EN
    print(f"ğŸ¤– Agent: {agent_type_enum.value}")
    
    # 1. WybÃ³r modeli do testowania
    print("\nğŸ“‹ WYBÃ“R MODELI DO TESTOWANIA:")
    print("1. Wszystkie modele z config.yaml")
    print("2. Tylko modele z tested: true bez wynikÃ³w w logach")
    mode_choice = input("Wybierz tryb: 1 - Wszystkie modele z config, 2 - Tylko tested: true bez wynikÃ³w: ")
    
    # 2. Czy automatycznie pobieraÄ‡ modele
    print("\nğŸ“¦ POBIERANIE MODELI:")
    auto_install = input("Czy automatycznie pobieraÄ‡ brakujÄ…ce modele? (y/n): ").lower().strip()
    install_choice = "y" if auto_install in ["y", "yes", "tak"] else "n"
    
    print("\n" + "="*60)
    print("âœ… KONFIGURACJA ZAKOÅƒCZONA")
    print("="*60)
    
    # Najlepsza optymalizacja z analizy
    best_optimization = [
        {},  # Baseline
        {"--flash-attn": None, "--cont-batching": None},  # Najlepsza kombinacja (-24.5% latencji)
    ]
    
    print(f"ğŸš€ Optymalizacje do testowania:")
    print(f"   1. Baseline (bez optymalizacji)")
    print(f"   2. Flash Attention + Continuous Batching (najlepsza: -24.5% latencji)")
    
    # Pobierz modele do testowania
    if mode_choice == "1":
        models_to_evaluate = EvalModelsReferenced.get_truly_untested_models(agent_type_enum.value, "referenced", only_tested_true=False)
        print("ğŸ“‹ Tryb: Wszystkie modele z config")
    else:
        models_to_evaluate = EvalModelsReferenced.get_truly_untested_models(agent_type_enum.value, "referenced", only_tested_true=True)
        print("ğŸ“‹ Tryb: Tylko tested: true bez wynikÃ³w")
    
    if not models_to_evaluate:
        print(f"âŒ Brak modeli do testowania dla agenta {agent_type_enum.value}")
        exit(1)
        
    total_models = len(models_to_evaluate)
    print(f"ğŸ“Š Znaleziono {total_models} modeli do testowania")

    # === PÄ˜TLA PRZEZ MODELE ===
    for i, model_name in enumerate(models_to_evaluate, 1):
        print(f"\n{'='*80}")
        print(f"ğŸ¯ MODEL {i}/{total_models}: {model_name}")
        print(f"{'='*80}")

        # SprawdÅº dostÄ™pnoÅ›Ä‡ modelu (uÅ¼ywaj globalnego ustawienia)
        if not BaseEvaluation.check_model_availability(model_name, install_choice=install_choice):
            print(f"â­ï¸  Pomijam model {model_name}...")
            continue
            
        print(f"âœ… Model dostÄ™pny: {model_name}")
        print(f"âœ… Agent: {agent_type_enum.value}")
        
        # === REFERENCED EVALUATION ===
        print(f"\nğŸ” REFERENCED EVALUATION - {model_name}")
        print(f"-" * 60)
        
        try:
            referenced_evaluator = EvalModelsReferenced(
                model_name=model_name,
                agent=agent_type_enum
            )
            
            # Uruchom referenced z najlepszymi optymalizacjami (tylko logi)
            referenced_evaluator.pipeline_eval_model(
                mode="logs_only",  # Tylko logi, bez wizualizacji
                use_cache=True,
                optimisations=best_optimization
            )
            
            print(f"âœ… Referenced evaluation zakoÅ„czona dla {model_name}")
            del referenced_evaluator  # Zwolnij pamiÄ™Ä‡
            
        except Exception as e:
            print(f"âŒ BÅ‚Ä…d w referenced evaluation dla {model_name}: {e}")
            continue  # PrzejdÅº do nastÄ™pnego modelu
        
        # === UNREFERENCED EVALUATION ===
        print(f"\nğŸ¥ UNREFERENCED EVALUATION - {model_name}")
        print(f"-" * 60)
        
        try:
            unreferenced_evaluator = EvalModelsUnreferenced(
                model_name=model_name,
                agent=agent_type_enum
            )
            
            # Uruchom unreferenced z najlepszymi optymalizacjami (tylko logi)
            unreferenced_evaluator.pipeline_eval_model(
                mode="logs_only",  # Tylko logi, bez wizualizacji
                use_cache=True,
                optimisations=best_optimization
            )
            
            print(f"âœ… Unreferenced evaluation zakoÅ„czona dla {model_name}")
            del unreferenced_evaluator  # Zwolnij pamiÄ™Ä‡
            
        except Exception as e:
            print(f"âŒ BÅ‚Ä…d w unreferenced evaluation dla {model_name}: {e}")
        
        # === OZNACZ MODEL JAKO TESTED ===
        try:
            from model_config_loader import mark_model_as_tested
            mark_model_as_tested(agent_type_enum.value, model_name)
            print(f"ğŸ’¾ Model {model_name} oznaczony jako tested: true")
        except Exception as e:
            print(f"âš ï¸ Nie udaÅ‚o siÄ™ oznaczyÄ‡ modelu jako tested: {e}")
        
        print(f"ğŸ‰ Model {model_name} zakoÅ„czony - referenced + unreferenced")

    # === PODSUMOWANIE ===
    print(f"\n{'='*80}")
    print(f"ğŸ† EWALUACJA STEP-BY-STEP ZAKOÅƒCZONA!")
    print(f"{'='*80}")
    print(f"ğŸ“Š Przetestowano {total_models} modeli")
    print(f"ğŸ”§ KaÅ¼dy model: Referenced â†’ Unreferenced")
    print(f"âš¡ Optymalizacje: Baseline + Flash Attention + Continuous Batching")
    print(f"ğŸ“ Tryb: Tylko logi (bez wizualizacji)")
    print(f"\nğŸ’¡ Aby wygenerowaÄ‡ wizualizacje, uruchom:")
    print(f"   - referenced_clean.py z mode='viz_only'")
    print(f"   - unreferenced_clean.py z mode='viz_only'")