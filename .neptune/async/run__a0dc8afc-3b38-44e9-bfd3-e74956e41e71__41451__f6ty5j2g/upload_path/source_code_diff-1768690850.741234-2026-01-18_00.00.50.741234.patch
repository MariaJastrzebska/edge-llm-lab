diff --git a/.neptune/async/run__a0dc8376-8b45-4b6f-831f-561584e0086b__37744__79i03kss/last_ack_version b/.neptune/async/run__a0dc8376-8b45-4b6f-831f-561584e0086b__37744__79i03kss/last_ack_version
deleted file mode 100644
index 87d78b2..0000000
--- a/.neptune/async/run__a0dc8376-8b45-4b6f-831f-561584e0086b__37744__79i03kss/last_ack_version
+++ /dev/null
@@ -1 +0,0 @@
-322
\ No newline at end of file
diff --git a/.neptune/async/run__a0dc8376-8b45-4b6f-831f-561584e0086b__37744__79i03kss/last_put_version b/.neptune/async/run__a0dc8376-8b45-4b6f-831f-561584e0086b__37744__79i03kss/last_put_version
deleted file mode 100644
index c954f9c..0000000
--- a/.neptune/async/run__a0dc8376-8b45-4b6f-831f-561584e0086b__37744__79i03kss/last_put_version
+++ /dev/null
@@ -1 +0,0 @@
-394
\ No newline at end of file
diff --git a/.neptune/async/run__a0dc8376-8b45-4b6f-831f-561584e0086b__37744__79i03kss/metadata.json b/.neptune/async/run__a0dc8376-8b45-4b6f-831f-561584e0086b__37744__79i03kss/metadata.json
deleted file mode 100644
index 437977c..0000000
--- a/.neptune/async/run__a0dc8376-8b45-4b6f-831f-561584e0086b__37744__79i03kss/metadata.json
+++ /dev/null
@@ -1,10 +0,0 @@
-{
-  "mode": "async",
-  "containerId": "a0dc8376-8b45-4b6f-831f-561584e0086b",
-  "containerType": "run",
-  "structureVersion": 3,
-  "os": "macOS-26.2-arm64-arm-64bit",
-  "pythonVersion": "3.10.14 (main, Mar 19 2024, 21:46:16) [Clang 15.0.0 (clang-1500.3.9.4)]",
-  "neptuneClientVersion": "1.14.0",
-  "createdAt": "2026-01-17T22:39:48.385402+00:00"
-}
\ No newline at end of file
diff --git a/examples/desktop/output/agents/constant_data_en/referenced/model/granite4_micro/run_004/baseline_round_0.png b/examples/desktop/output/agents/constant_data_en/referenced/model/granite4_micro/run_004/baseline_round_0.png
index 7d51965..de33580 100644
Binary files a/examples/desktop/output/agents/constant_data_en/referenced/model/granite4_micro/run_004/baseline_round_0.png and b/examples/desktop/output/agents/constant_data_en/referenced/model/granite4_micro/run_004/baseline_round_0.png differ
diff --git a/examples/desktop/output/agents/constant_data_en/referenced/model/granite4_micro/run_004/baseline_round_1.png b/examples/desktop/output/agents/constant_data_en/referenced/model/granite4_micro/run_004/baseline_round_1.png
index bc861fa..c357e26 100644
Binary files a/examples/desktop/output/agents/constant_data_en/referenced/model/granite4_micro/run_004/baseline_round_1.png and b/examples/desktop/output/agents/constant_data_en/referenced/model/granite4_micro/run_004/baseline_round_1.png differ
diff --git a/examples/desktop/output/agents/constant_data_en/referenced/model/granite4_micro/run_004/baseline_round_2.png b/examples/desktop/output/agents/constant_data_en/referenced/model/granite4_micro/run_004/baseline_round_2.png
index edc35ac..142a27b 100644
Binary files a/examples/desktop/output/agents/constant_data_en/referenced/model/granite4_micro/run_004/baseline_round_2.png and b/examples/desktop/output/agents/constant_data_en/referenced/model/granite4_micro/run_004/baseline_round_2.png differ
diff --git a/examples/desktop/output/agents/constant_data_en/referenced/model/granite4_micro/run_004/baseline_round_3.png b/examples/desktop/output/agents/constant_data_en/referenced/model/granite4_micro/run_004/baseline_round_3.png
index b8368ff..499300f 100644
Binary files a/examples/desktop/output/agents/constant_data_en/referenced/model/granite4_micro/run_004/baseline_round_3.png and b/examples/desktop/output/agents/constant_data_en/referenced/model/granite4_micro/run_004/baseline_round_3.png differ
diff --git a/examples/desktop/output/agents/constant_data_en/referenced/model/granite4_micro/run_004/baseline_round_4.png b/examples/desktop/output/agents/constant_data_en/referenced/model/granite4_micro/run_004/baseline_round_4.png
index d3703f6..f4b5156 100644
Binary files a/examples/desktop/output/agents/constant_data_en/referenced/model/granite4_micro/run_004/baseline_round_4.png and b/examples/desktop/output/agents/constant_data_en/referenced/model/granite4_micro/run_004/baseline_round_4.png differ
diff --git a/src/edge_llm_lab/evaluation/referenced_evaluator.py b/src/edge_llm_lab/evaluation/referenced_evaluator.py
index 2cd7724..faabdea 100755
--- a/src/edge_llm_lab/evaluation/referenced_evaluator.py
+++ b/src/edge_llm_lab/evaluation/referenced_evaluator.py
@@ -564,10 +564,16 @@ class EvalModelsReferenced(BaseEvaluation):
         output_dir = session_locations["all_models_output_directory"]
         # Generate a single timestamp for all files in this run
         metadata=self.all_model_metadata
-        print("DEBUG:  valid_session_data")
+        use_polish = getattr(self, 'use_polish', True)
         # Aggregate ALL models and ALL optimizations into ONE plot
-        # Don't group by optimization - use ALL data together
-        all_session_data = list(valid_session_data.values())
+        all_session_data = []
+        for session in valid_session_data.values():
+            m_name = session.get('model_name', '').lower()
+            # Strict filter for MOE models as they distort statistics
+            if 'moe' in m_name:
+                print(f"‚è© Globally filtering out failed MOE model from analysis: {m_name}")
+                continue
+            all_session_data.append(session)
 
         # Pozw√≥l u≈ºytkownikowi wybraƒá grupƒô lub konkretny model
         selected_session_data, selected_model_name = self.display_models_and_get_selection(all_session_data, interactive=interactive)
@@ -636,12 +642,19 @@ class EvalModelsReferenced(BaseEvaluation):
             print(f"üèÜ Category winners saved to: {mobile_plots['category_winners']}")
             
         # 5. Generate Resource Health Check (RAM/SWAP/Throttling) for all models
-        self.plot_resource_health_check(all_session_data, output_dir, timestamp)
+        try:
+            print("\nü©∫ Generating resource health check diagnostics...")
+            self.plot_resource_health_check(all_session_data, output_dir, timestamp)
+        except Exception as e:
+            print(f"‚ùå Error in plot_resource_health_check: {str(e)}")
         
         # 6. Generate individual throttling timelines for each model
-        print("\nüìà Generating individual throttling timelines...")
-        for session in all_session_data:
-            self.plot_throttling_timeline(session, output_dir, timestamp)
+        try:
+            print("\nüìà Generating individual throttling timelines...")
+            for session in all_session_data:
+                self.plot_throttling_timeline(session, output_dir, timestamp)
+        except Exception as e:
+            print(f"‚ùå Error in plot_throttling_timeline: {str(e)}")
             
 
                 
@@ -2777,6 +2790,10 @@ class EvalModelsReferenced(BaseEvaluation):
                     continue
                     
                 model_name_item = model_session.get('model_name', 'unknown_model')
+                 # Skip failed models (Granite MoE)
+                model_name_lower = model_name_item.lower()
+                if 'granite3-moe' in model_name_lower or 'moe' in model_name_lower:
+                    continue
                 
                 # Initialize model data if not exists
                 if model_name_item not in models_data:
@@ -2902,8 +2919,8 @@ class EvalModelsReferenced(BaseEvaluation):
             avg_cpu_power = np.mean(data['cpu_power']) if data['cpu_power'] else 0
             avg_gpu_power = np.mean(data['gpu_power']) if data['gpu_power'] else 0
             
-            # Only add if we have valid data
-            if avg_gpt > 0 or avg_lat > 0:
+            # Only add if we have valid data (filter out models with score <= 1.0)
+            if avg_gpt > 1.0:
                 model_names.append(model_name)
                 avg_scores.append(avg_gpt)
                 avg_latencies.append(avg_lat)
@@ -3233,13 +3250,17 @@ class EvalModelsReferenced(BaseEvaluation):
         
         # Plot each model with consistent colors
         for i, model in enumerate(model_names):
-            # Scale each metric to 0-1 with absolute scales
-            # GPT Score: scale between min-max (higher score = better = closer to edge)
-            if MAX_GPT_SCORE > MIN_GPT_SCORE:
-                gpt_score_scaled = (avg_scores[i] - MIN_GPT_SCORE) / (MAX_GPT_SCORE - MIN_GPT_SCORE)
-                gpt_score_scaled = max(0, min(1, gpt_score_scaled))
-            else:
-                gpt_score_scaled = 1.0
+            # Scale each metric to 0-1
+            # GPT Score: Use absolute 0-10 scale
+            # To provide better visual separation for high scores (e.g. 9.1 vs 9.4), 
+            # we can use a slightly shifted scale if all scores are very high, 
+            # but for now let's stick to absolute 0-10 as requested.
+            gpt_score_scaled = avg_scores[i] / 10.0
+            gpt_score_scaled = max(0, min(1, gpt_score_scaled))
+            
+            # To make differences more visible if clustering at the edge:
+            # gpt_score_scaled = (avg_scores[i] - 5) / 5  # Map 5-10 to 0-1
+            # But user asked for absolute 0-10.
             
             # Latency: scale between min-max (lower latency = better = closer to edge)
             latency_ms = avg_latencies[i]
@@ -3276,8 +3297,8 @@ class EvalModelsReferenced(BaseEvaluation):
             values = [gpt_score_scaled, latency_scaled, size_scaled, power_scaled]
             values += values[:1]  # Close the loop
             
-            # Debug radar values (commented out for cleaner output)
-            # print(f"DEBUG RADAR VALUES for {model}: GPT={avg_scores[i]:.3f}‚Üí{gpt_score_scaled:.2f}, Latency={latency_ms:.0f}ms‚Üí{latency_scaled:.2f}, Size={model_sizes[i]:.1f}GB‚Üí{size_scaled:.2f}, Power={model_power:.0f}mW‚Üí{power_scaled:.2f}")
+            # Debug radar values - uncommented to diagnose the "all maximum" issue
+            print(f"üéØ DEBUG RADAR VALUES: Model={model}, GPT_raw={avg_scores[i]:.3f}, GPT_scaled={gpt_score_scaled:.3f}, Latency={latency_ms:.0f}ms‚Üí{latency_scaled:.2f}, Size={model_sizes[i]:.1f}GB‚Üí{size_scaled:.2f}, Power={model_power:.0f}mW‚Üí{power_scaled:.2f}")
             
             # Use consistent color for this model
             color = model_color_map[model]
@@ -3295,7 +3316,7 @@ class EvalModelsReferenced(BaseEvaluation):
         
         if use_polish:
             detailed_categories = [
-                f'Wynik GPT\n({MIN_GPT_SCORE:.1f}-{MAX_GPT_SCORE:.1f})',
+                f'Wynik GPT\n(0-10)',
                 f'Latencja\n({MIN_MOBILE_LATENCY_MS:.0f}-{MAX_MOBILE_LATENCY_MS:.0f}ms)',
                 f'Rozmiar modelu\n({MIN_MOBILE_SIZE_GB:.1f}-{MAX_MOBILE_SIZE_GB:.1f}GB)',
                 f'Efektywno≈õƒá energetyczna\n({MIN_TOTAL_POWER:.0f}-{MAX_TOTAL_POWER:.0f}mW)'
@@ -3303,7 +3324,7 @@ class EvalModelsReferenced(BaseEvaluation):
             title_text = 'Radar Wydajno≈õci Mobilnej\n(Zewnƒôtrzna krawƒôd≈∫ = Lepsze)'
         else:
             detailed_categories = [
-                f'GPT Score\n({MIN_GPT_SCORE:.1f}-{MAX_GPT_SCORE:.1f})',
+                f'GPT Score\n(0-10)',
                 f'Latency\n({MIN_MOBILE_LATENCY_MS:.0f}-{MAX_MOBILE_LATENCY_MS:.0f}ms)',
                 f'Model Size\n({MIN_MOBILE_SIZE_GB:.1f}-{MAX_MOBILE_SIZE_GB:.1f}GB)',
                 f'Energy Efficiency\n({MIN_TOTAL_POWER:.0f}-{MAX_TOTAL_POWER:.0f}mW)'
@@ -3311,9 +3332,9 @@ class EvalModelsReferenced(BaseEvaluation):
             title_text = 'Mobile Performance Radar\n(Outer edge = Better)'
         
         ax.set_xticklabels(detailed_categories)
-        ax.set_title(title_text, pad=20, fontsize=12, fontweight='bold')
-        # Add legend for easier model identification
-        ax.legend(loc='upper right', bbox_to_anchor=(1.3, 1.1), fontsize=9)
+        ax.set_title(title_text, pad=30, fontsize=12, fontweight='bold')
+        # Add legend further outside to avoid overlapping
+        ax.legend(loc='upper left', bbox_to_anchor=(1.2, 1.0), fontsize=8, frameon=True)
         
         # Set y-axis limits but remove percentage labels
         ax.set_ylim(0, 1)
@@ -3397,35 +3418,27 @@ class EvalModelsReferenced(BaseEvaluation):
                 edgecolors='black', linewidth=0.5
             )
         
-        # Set axis limits with padding
-        if latencies_ms and avg_scores:
-            x_padding = (max(latencies_ms) - min(latencies_ms)) * 0.2
-            y_padding = (max(avg_scores) - min(avg_scores)) * 0.2
-            
-            ax.set_xlim(max(0, min(latencies_ms) - x_padding), 
-                       max(latencies_ms) + x_padding)
-            ax.set_ylim(max(0, min(avg_scores) - y_padding), 
-                       min(100, max(avg_scores) + y_padding))  # Cap at 100% for scores
-        
-        # Add labels and title
-        # Check language preference
-        try:
-            use_polish = getattr(self, '_current_use_polish', True)
-        except:
-            use_polish = True
-            
         if use_polish:
             ax.set_xlabel('≈örednia latencja (ms) ‚Üí Ni≈ºej lepiej')
-            ax.set_ylabel('≈öredni wynik GPT (%) ‚Üí Wy≈ºej lepiej')
+            ax.set_ylabel('≈öredni wynik GPT (0-10) ‚Üí Wy≈ºej lepiej')
             ax.set_title('Wydajno≈õƒá modelu vs Latencja\n(Rozmiar bƒÖbelka ‚àù Rozmiar modelu)')
         else:
             ax.set_xlabel('Average Latency (ms) ‚Üí Lower is better')
-            ax.set_ylabel('Average GPT Score (%) ‚Üí Higher is better')
+            ax.set_ylabel('Average GPT Score (0-10) ‚Üí Higher is better')
             ax.set_title('Model Performance vs Latency\n(Bubble size ‚àù Model Size)')
         ax.grid(True, linestyle='--', alpha=0.6)
         
-        # Add legend for model colors
-        # Add legend for model colors with standardized sizes
+        # Set axis limits with padding for 0-10 scale
+        if latencies_ms and avg_scores:
+            x_padding = (max(latencies_ms) - min(latencies_ms)) * 0.2 if len(latencies_ms) > 1 else 100
+            y_padding = (max(avg_scores) - min(avg_scores)) * 0.2 if len(avg_scores) > 1 else 1
+            
+            ax.set_xlim(max(0, min(latencies_ms) - x_padding), 
+                       max(latencies_ms) + x_padding)
+            ax.set_ylim(max(0, min(avg_scores) - y_padding), 
+                       min(10.5, max(avg_scores) + y_padding))
+        
+        # Add legend for model colors with standardized sizes - Move outside
         ax.legend(handles=legend_elements, bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=8)
 
     def _shorten_model_labels(self, model_names):
@@ -4927,6 +4940,7 @@ class EvalModelsReferenced(BaseEvaluation):
         swap_used = []
         cpu_freq = []
         gpu_util = []
+        cpu_freq_max = 0
 
         for r in session['rounds']:
             lb = r.get('latency_breakdown', {})
@@ -4958,7 +4972,7 @@ class EvalModelsReferenced(BaseEvaluation):
         ax1.legend()
 
         # Plot 2: CPU Frequency (Throttling)
-        if cpu_freq_max:
+        if cpu_freq_max > 0:
             cpu_freq_pct = [f/cpu_freq_max*100 for f in cpu_freq]
             ax2.plot(rounds_idx, cpu_freq_pct, '^-', color='#f1c40f', label='CPU Freq (%)', linewidth=2)
             ax2.axhline(y=100, color='gray', linestyle=':', alpha=0.5)
@@ -5139,6 +5153,10 @@ class EvalModelsReferenced(BaseEvaluation):
                     continue
                     
                 model_name = model_session.get('model_name', 'unknown_model')
+                # Skip failed models (Granite MoE)
+                if 'granite3-moe' in model_name.lower() or 'moe' in model_name.lower():
+                    continue
+                    
                 rounds = model_session.get('rounds', [])
                 
                 
@@ -5386,13 +5404,13 @@ class EvalModelsReferenced(BaseEvaluation):
             if use_polish:
                 ax2.set_xlabel('≈örednia latencja (ms)')
                 ax2.set_ylabel('Przepustowo≈õƒá (tokeny/sekundƒô)')
-                ax2.set_title('Korelacja rundy vs Ca≈Çkowita latencja (Rozmiar bƒÖbelka = Rozmiar modelu)')
+                ax2.set_title('Korelacja TPS vs Latencja (Rozmiar bƒÖbelka ‚àù Rozmiar modelu)')
             else:
                 ax2.set_xlabel('Average Latency (ms)')
-                ax2.set_ylabel('Throughput (tokens/second)')
-                ax2.set_title('Round vs Total Latency Correlation (Bubble Size = Model Size)')
+                ax2.set_ylabel('Throughput (tokens/sec)')
+                ax2.set_title('TPS vs Latency Correlation (Bubble size ‚àù Model Size)')
             ax2.grid(True, alpha=0.3)
-            
+            # Legend is redundant here since bar chart above identifies models
             # Add legend for bubble sizes
             size_legend_elements = []
             if model_sizes_gb:
@@ -5416,9 +5434,7 @@ class EvalModelsReferenced(BaseEvaluation):
                     Line2D([0], [0], marker='o', color='w', markerfacecolor='gray', markersize=16, alpha=0.6, label=f'{max_size_gb:.1f}GB')
                 ]
                 
-                # Combine both legends
-                leg1 = ax2.legend(handles=color_legend_elements, title='Models', loc='lower right', fontsize=8)
-                ax2.add_artist(leg1)
+                # Only add legend for Model Size since model names are in the bar chart above
                 ax2.legend(handles=size_legend_elements, title='Model Size', loc='upper right', fontsize=8)
             
             # Adjust layout and save
